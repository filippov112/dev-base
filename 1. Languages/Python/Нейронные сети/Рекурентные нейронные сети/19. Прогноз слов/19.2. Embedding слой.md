#ML

# Работа с Embedding слоем в нейронных сетях с использованием Keras

## Описание проблемы
В первой реализации есть один существенный недостаток: входной тензор, который мы получили, занимает в памяти очень много места.

## Описание Embedding слоя
Embedding - специальный входной слой, который:
- Подает 1 на входной нейрон с соответствующим номером слова, которое мы хотим подать на вход,
- А остальные суммы приравнивает к нулю.
- На выходах его слоя формируются выходные значения, равные весам связей (связи также подвергаются обучению) для переданной 1.
- Далее, эти значения весов подаются уже на следующий слой нейронной сети.

## Пример создания Embedding слоя в Keras
```python
from tensorflow.keras.layers import Dense, SimpleRNN, Embedding
from tensorflow.keras.models import Sequential

model = Sequential()
model.add(Embedding(input_dim=maxWordsCount, output_dim=256, input_length=inp_words))
model.add(SimpleRNN(128, activation='tanh'))
model.add(Dense(maxWordsCount, activation='softmax'))
model.summary()

model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')
history = model.fit(X, Y, batch_size=32, epochs=50)
```

## Прогнозирование с использованием модели
```python
def buildPhrase(texts, str_len=20):
    res = texts
    data = tokenizer.texts_to_sequences([texts])[0]
    for i in range(str_len):
        x = data[i: i + inp_words]
        inp = np.expand_dims(x, axis=0)
        pred = model.predict(inp)
        indx = pred.argmax(axis=1)[0]
        data.append(indx)
        res += " " + tokenizer.index_word[indx]
    return res

# Пример использования функции buildPhrase
res = buildPhrase("позитив добавляет годы")
print(res)
```

## Импорты и формирование выборки
```python
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import numpy as np
from tensorflow.keras.layers import Dense, SimpleRNN, Input, Embedding
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence
from tensorflow.keras.utils import to_categorical

# Формирование выборки
with open('text', 'r', encoding='utf-8') as f:
    texts = f.read()
    texts = texts.replace('\ufeff', '')

maxWordsCount = 1000
tokenizer = Tokenizer(num_words=maxWordsCount, 
                      filters='!–"—#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n\r«»',
                      lower=True, split=' ', char_level=False)

tokenizer.fit_on_texts([texts])
data = tokenizer.texts_to_sequences([texts])
res = np.array(data[0])

inp_words = 3
n = res.shape[0] - inp_words

X = np.array([res[i:i + inp_words] for i in range(n)])
Y = to_categorical(res[inp_words:], num_classes=maxWordsCount)
```
