#ML

# Двунаправленные слои в нейронных сетях

Двунаправленные слои выполняют обработку входной цепочки сигналов двумя подслоями:
1. Первый подслой: от начала до конца.
2. Второй подслой: с конца в начало (запоминает последовательность входных данных).

После этого формируется выходное значение из выходных данных обоих подслоев для соответствующих входных сигналов.

![](1.%20Languages/Python/Нейронные%20сети/Рекурентные%20нейронные%20сети/24.1.%20Схема%20двунаправленной%20РНС.png)

## Пример применения двунаправленных слоев для прогнозирования слова по окружающему контексту

### Импорты

```python
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.layers import Dense, GRU, Input, Dropout, Bidirectional
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
```

### Формировка выборки

```python
N = 10000
data = np.array([np.sin(x*0.2) for x in range(N)]) + 0.1*np.random.randn(N) # создаем массив сигмоидальной ф-ции со случайными помехами

off = 3 # разброс (+-3 слова)
length = off*2 + 1 # общая длина выборки (включая прогнозируемое, которое не включается)

X = np.array([np.diag(np.hstack((data[i+off], data[i+off+1:i+length]))) for i in range(N-length)]) # отбираем для каждого элемента выборки массив из его окружения
Y = data[off:N-off] # сама выборка в качестве выходного массива
```

### Оформление нейронной сети

```python
model = Sequential()
model.add(Input((length-1, length-1)))
model.add(Bidirectional(GRU(2)))
model.add(Dense(1, activation='linear'))
model.summary()

model.compile(loss='mean_squared_error', optimizer=Adam(0.01))
history = model.fit(X, Y, batch_size=32, epochs=10)
```

### Проверка

```python
M = 200
XX = np.zeros(M)
XX[off] = data[off]
for i in range(M-off-1):
    x = np.diag(np.hstack((XX[i+off], data[i+off+1:i+length])))
    x = np.expand_dims(x, axis=0)
    y = model.predict(x)
    XX[i+off] = y

plt.plot(XX[:M])
plt.plot(data[:M])
```
