#ML

# Batch Normalization

Алгоритм batch normalization приводит к единому виду статистические характеристики распределения на выходах нейронов. А именно, математическое ожидание и дисперсию. Их приводят к значениям 0 и 1, то есть, формируют распределение с нулевым МО и единичной дисперсией. Данная нормировка производится для входящих сумм нейронов в пределах каждого батча (по стандарту).

МО - вычисляется как среднее значение входящей суммы в пределах батча:
`[ m = frac{v1 + ldots + vN}{N} ]`

Дисперсия - вычисляется как средний квадрат разности реальной суммы и МО:
`[ text{dis}^2 = frac{(v1 - m)^2 + ldots + (vN - m)^2}{N} ]`

Далее каждая входящая сумма преобразовывается к "около-единичному" формату:
`[ z = frac{v - m}{sqrt{text{dis}^2 + 0.000001}} ]`
где ( 0.000001 ) - константа против деления на 0.

Получившееся значение поступает в нейрон через функцию масштабирования, которая в свою очередь настраивается нейросетью по ходу обучения:
`[ y = a cdot z + b ]`
где изначально ( a = 1 ) и ( b = 0 ).

---

### Реализация batch normalization в Keras

Он применяется к выходам предыдущего слоя, после которого указан в модели нейронной сети, например:

```python
model = keras.Sequential([
    Flatten(input_shape=(28, 28, 1)),
    Dense(300, activation='relu'),
    BatchNormalization(),  # вот он
    Dense(10, activation='softmax')
])
```
