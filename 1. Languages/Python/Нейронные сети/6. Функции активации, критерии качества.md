#ML

# Функции активации, критерии качества

## ПРАВИЛО 1. Функции активации скрытых слоев

При малом числе слоев можно использовать гиперболическую и сигмоидальную функции активации или ReLU. При числе слоев от 8 и более – использовать ReLU и ее вариации.

Функция активации ReLU: ( f(x) = max(0, x) )

---

ОДНАКО, для ВЫХОДНОГО слоя функция активации часто меняется и выбирается другой.

## ПРАВИЛО 2. Функции активации выходных нейронов

- Для задач регрессии у выходных нейронов используется линейная (linear) функция активации.
- Для задач классификации не пересекающихся классов – softmax.

Линейная функция активации (linear): ( `f(x) = x` )

Softmax: (` f_i(x) = frac{e^{x_i}}{sum_{j} e^{x_j}}` ) (для всех нейронов ( i ))

---

## ФУНКЦИИ ОЦЕНКИ КРИТЕРИЯ КАЧЕСТВА ДЛЯ РАЗНЫХ ЗАДАЧ

### Распознавание:

НС имеет несколько выходов M, на каждом из которых появляются значения +1 или -1.

- Хиндж (hinge).
- Бинарная кросс-энтропия (binary crossentropy) – при классификации двух классов.
- Категориальная кросс-энтропия (categorical crossentropy) – при классификации более двух классов.

### Обработка текста:

- Логарифмический гиперболический косинус (logcosh).

### Задачи регрессии:

Выходное значение нейрона представлено в числовой шкале (цены, рост, вес, количества и т.п.).

- Средний квадрат ошибок (mean squared error).
- Средний модуль ошибок (mean absolute error).
- Средний абсолютный процент ошибок (mean absolute percentage error) – хорош при прогнозировании.
- Средний квадрат логарифмических ошибок (mean squared logarithmic error).


![6. Функции оценки критерия качества](1.%20Languages/Python/Нейронные%20сети/6.%20Функции%20оценки%20критерия%20качества/6.%20Функции%20оценки%20критерия%20качества.md)
