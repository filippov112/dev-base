#theory #theory-ml
 
---
```python
import numpy as np

def f(x):  # функция активатор
    return 2/(1 + np.exp(-x)) - 1

def df(x):  # производная от функции активатора
    return 0.5*(1 + x)*(1 - x)

W1 = np.array([[-0.2, 0.3, -0.4], [0.1, -0.3, -0.4]])  # начальные веса связей для нейронов (W1, W2 - слои)
W2 = np.array([0.2, 0.3])  # случайные значения, определяющие степень влияния фактора нейрона (от -1 до +1)

def go_forward(inp):  # функция прогонки (inp - сигнал)
    sum = np.dot(W1, inp)  # загоняем сигнал на вход первого слоя
    out = np.array([f(x) for x in sum])  # преобразуем сигнал функцией активации

    sum = np.dot(W2, out)  # аналогично поступаем со следующим слоем
    y = f(sum)  # на выходе получим только 1 значение (массив не нужен)
    return (y, out)  # возвращаем результат прогонки и выходные сигналы нейронов всех (одного) слоев

def train(epoch):  # функция обучения (принимает список 3-значных сигналов с требуемыми результатами работы НС)
    global W2, W1
    lmd = 0.01  # шаг обучения
    N = 10000  # число итераций при обучении
    count = len(epoch)
    for k in range(N):
        x = epoch[np.random.randint(0, count)]  # случайный выбор входного сигнала из обучающей выборки
        y, out = go_forward(x[0:3])  # прямой проход по НС и вычисление выходных значений нейронов
        e = y - x[-1]  # ошибка
        delta = e * df(y)  # локальный градиент

        # корректировка весов 1 единственного нейрона последнего слоя
        W2[0] = W2[0] - lmd * delta * out[0]  # корректировка веса первой связи
        W2[1] = W2[1] - lmd * delta * out[1]  # корректировка веса второй связи

        delta2 = W2 * delta * df(out)  # вектор из 2-х величин локальных градиентов

        # корректировка связей первого слоя
        W1[0, :] = W1[0, :] - np.array(x[0:3]) * delta2[0] * lmd
        W1[1, :] = W1[1, :] - np.array(x[0:3]) * delta2[1] * lmd

        # повторяем процесс (случайный сигнал - прогонка - корректировка) большое количество раз - (10000 в нашем случае)

epoch = [(-1, -1, -1, -1),  # обучающая выборка (она же полная выборка)
         (-1, -1, 1, 1),
         (-1, 1, -1, -1),
         (-1, 1, 1, 1),
         (1, -1, -1, -1),
         (1, -1, 1, 1),
         (1, 1, -1, -1),
         (1, 1, 1, -1)]
train(epoch)  # запуск обучения сети

for x in epoch:  # проверка полученных результатов (ручная прогонка всех сигналов)
    y, out = go_forward(x[0:3])
    print(f"Выходное значение НС: {y} => {x[-1]}")
```
