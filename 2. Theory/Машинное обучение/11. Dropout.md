

# Dropout

При обучении сети может возникнуть ситуация, когда переобучение начнется раньше, чем сеть достигнет своего пикового показателя качества, при этом числе нейронов. А при уменьшении их (нейронов) числа, пиковый показатель качества сети будет становиться ниже. Чтобы отодвинуть момент начала переобучения сети можно использовать метод Дропаута, для увеличения обобщающей способности сети на том же уровне, нежели без него.

**Dropout** - на каждой итерации изменения весовых коэффициентов часть нейронов нужно исключать с заданной вероятностью \( p \). Если после отключения некоторой части нейронов перед итерацией, осталось условно 0.8 от их изначального числа, то выходной сигнал мы должны разделить на эти самые 0.8, дабы сохранить его начальный размер.

---

**РЕАЛИЗАЦИЯ В KERAS**

```python
model = keras.Sequential([
    Flatten(input_shape=(28, 28, 1)),
    Dense(300, activation='relu'),
    Dropout(0.8),    # отключение 80% нейронов предыдущего слоя (300 relu)
    Dense(10, activation='softmax')
])
```
