

# Особенности Back Propagation

## Какую функцию активации нейронов выбрать

### ПРАВИЛО 1. НАЧАЛЬНЫЕ ВЕСА СВЯЗЕЙ
Запускать алгоритм для разных начальных значений весовых коэффициентов. И, затем, отобрать лучший вариант. Начальные значения генерируем случайным образом в окрестности нуля, кроме тех, что относятся к bias’ам.

### ПРАВИЛО 2. ОПТИМИЗАЦИЯ ДЛЯ ПОЛОГИХ УЧАСТКОВ
Запускаем алгоритм обучения с оптимизацией по Adam или Нестерову для ускорения обучения НС.

---

#### Проблема градиентных алгоритмов – медленная сходимость на пологих участках функции:
**Решения:**
- оптимизация на основе моментов (momentum);
- ускоренные градиенты Нестерова (nesterov momentum);
- метод Adagrad;
- методы RMSProp и Adadelta;
- метод Adam и NAdam.

---

### ПРАВИЛО 3. НОРМИРОВКА ВХОДНЫХ ДАННЫХ
Выполнять нормировку входных значений и запоминать нормировочные параметры `min`, `max` из обучающей выборки.
Формула: `(Xi - Min) / (Max - Min)`, где `Xi` - перебираемые факторы входного сигнала.

### ПРАВИЛО 4. ВЫБОРКА
Помещать в обучающую выборку самые разнообразные данные примерно равного количества.

### ПРАВИЛО 5. КОРРЕКТИРОВКА ВЕСОВ
Наблюдения на вход сети подавать случайным образом, корректировать веса после серии наблюдений, разбитых на `mini-batch`. Примечание: желательно использовать при большом количестве наблюдений, от 1000 в каждом `mini-batch`.

### ПРАВИЛО 6. КРИТЕРИИ КАЧЕСТВА
Значение критерия качества вычисляется только после прогонки всей эпохи (прогонки). Если оно нас не устраивает (как правило, так и есть), то наблюдения снова тасуются случайным образом и обучение продолжается. Качество определяется процентом ошибок НС. В конце каждой эпохи снова и снова пересчитывается критерий качества.
