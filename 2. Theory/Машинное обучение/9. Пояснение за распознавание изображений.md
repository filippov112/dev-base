#theory #theory-ml
 
---
Вектор входного сигнала состоит из пикселей изображения, 
где цифра выделяется яркостью пикселей(увеличенным значением цвета).

Нейроны первого скрытого слоя по разному реагируют на соответствующие входы.
Разница возникает за счет случайного распределения весов входящих связей.

За счет ReLu функции, мы получаем эффект, когда в зависимости от того,
насколько много приоритеных входов конкретного нейрона были задействованы в изображении,
этот нейрон подаёт более слабый или более сильный сигнал(прямая зависимость).

Нейроны выходного слоя так же по разному реагируют на разные нейроны скрытого слоя.
Каждый нейрон отвечает за свою определяемую цифру.

В процессе обучения сети, отталкиваясь от масштаба ошибки нейрона, её направления, 
силы сигнала нейронов предыдущего уровня, скорости изменения активационной функции 
в районе ошибки вносим соответствующие изменения в степени влияния нейронов.