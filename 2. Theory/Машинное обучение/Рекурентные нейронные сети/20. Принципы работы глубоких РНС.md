

# Принципы работы глубоких РНС

Число архитектур рекуррентных НС огромное множество, самых разных конфигураций. Наиболее известные (из однонаправленных), построенных на основе простейшей рекуррентной сети, следующие:

- **Deep Transition RNN**: несколько слоев в одном цикле.
- **Deep Transition, Deep Output RNN**: то же самое и глубокая сеть в конце.
- **Stacked RNN**: несколько последовательных слоев, каждый со своим циклом.

### Пример использования Stacked RNN

В пакете Keras на вход каждого рекуррентного слоя должен подаваться тензор размерами `(batch_size, timesteps, units)`:

- `batch_size`: размер пакета данных.
- `timesteps`: размер последовательности в каждом примере (длина связанной цепочки).
- `units`: число нейронов в RNN-слое.

### Исходная программа (19 уроков)

```python
model = Sequential()
model.add(Embedding(maxWordsCount, 128, input_length=inp_words))
model.add(SimpleRNN(128))
model.add(Dense(maxWordsCount, activation='softmax'))
model.summary()
```

В этой программе использовался слой типа `Many-to-One`. Из группы слов получается одно конечное слово, на основе которого вычисляется следующее.

### После доработки

```python
model = Sequential()
model.add(Embedding(maxWordsCount, 128, input_length=inp_words))
model.add(SimpleRNN(128, return_sequences=True))  # Параметр return_sequences устанавливает Many-to-Many.
model.add(SimpleRNN(64))
model.add(Dense(maxWordsCount, activation='softmax'))
model.summary()
```

Теперь, после каждой итерации первого слоя, выходной сигнал не только отправляется на следующую итерацию, но и дублируется на второй слой, который параллельно обрабатывает его и ожидает следующего дубликата первого слоя как свой второй элемент итерации.
