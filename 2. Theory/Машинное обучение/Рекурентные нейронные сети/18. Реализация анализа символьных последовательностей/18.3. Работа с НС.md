 #theory #theory-ml
 
---
##### 6. Пропишем структуру сети
```python
	model = Sequential()
	model.add(Input((inp_chars, num_characters))) 		# размеры входа 3х34
	model.add(SimpleRNN(500, activation='tanh')) 		# units – число нейронов рекуррентного слоя;
							  	 # activation – функция активации нейронов (по ум. – tanh (гиперболический тангенс))
	model.add(Dense(num_characters, activation='softmax'))	# выход 34 нейрона на softmax
	model.summary()
```

##### 7. Cкомпилируем ее и обучим по нашей выборке:
```python
	model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')
	history = model.fit(X, Y, batch_size=32, epochs=100)		# градиенты применяются после каждого прохождения по алфавиту

```


##### 8. Далее, объявим вспомогательную функцию, в которой будет выполняться прогноз очередного символа и добавления его в конец начальной строки:
```python
def buildPhrase(inp_str, str_len = 50):
	for i in range(str_len):
	x = []
	for j in range(i, i+inp_chars):	# здесь мы используем вложенный цикл, чтобы после каждого прогноза отбор символов для сети сдвигался по строке на 1 символ
		x.append(tokenizer.texts_to_matrix(inp_str[j])) # преобразуем символы в тензор 
														# алфавита (обращение символ-
														# >тензор)
														
		x = np.array(x)					 # переводим массив в numpy
	inp = x.reshape(1, inp_chars, num_characters)	# добавляем ось батчей
	pred = model.predict( inp ) 			# предсказываем тензор четвертого символа
	d = tokenizer.index_word[pred.argmax(axis=1)[0]] 	# получаем ответ в символьном представлении (обращение индекс->символ) 
	inp_str += d 					# дописываем строку
  return inp_str

```

##### 9. Проверка
```python
res = buildPhrase("утренн")
print(res)
```




