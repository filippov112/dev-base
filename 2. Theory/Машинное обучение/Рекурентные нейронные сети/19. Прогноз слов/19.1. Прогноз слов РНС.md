 #theory #theory-ml
 
---
Пример на основе программы 18 урока
[18. Реализация анализа символьных последовательностей](2.%20Theory/Машинное%20обучение/Рекурентные%20нейронные%20сети/18.%20Реализация%20анализа%20символьных%20последовательностей/18.%20Реализация%20анализа%20символьных%20последовательностей.md)
### Импорты
```python
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import numpy as np
from tensorflow.keras.layers import Dense, SimpleRNN, Input
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence
from tensorflow.keras.utils import to_categorical
```

### Подгрузка текста
```python
with open('text', 'r', encoding='utf-8') as f:
    texts = f.read()
    texts = texts.replace('\ufeff', '')  # убираем первый невидимый символ
```

### Формирование выборки
```python
maxWordsCount = 1000  # размер словаря
tokenizer = Tokenizer(num_words=maxWordsCount, 
                      filters='!–"—#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n\r«»',
                      lower=True, split=' ', char_level=False)
tokenizer.fit_on_texts([texts])  # формируем словарь из текста
data = tokenizer.texts_to_sequences([texts])  # заменяем слова в тексте на индексы слов в словаре
res = to_categorical(data[0], num_classes=maxWordsCount)  # преобразуем индексы в тензоры размером maxWordsCount
print(res.shape)
```

### Формирование вход/выход тензоров
```python
inp_words = 3
n = res.shape[0] - inp_words

X = np.array([res[i:i + inp_words] for i in range(n)])  # формируем входные данные
Y = res[inp_words:]  # формируем выходные данные
```

### Инициализация Нейронной Сети
```python
model = Sequential()
model.add(Input((inp_words, maxWordsCount)))
model.add(SimpleRNN(128, activation='tanh'))
model.add(Dense(maxWordsCount, activation='softmax'))
model.summary()

model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')
history = model.fit(X, Y, batch_size=32, epochs=50)
```

### Прогнозирование текста
```python
def buildPhrase(texts, str_len=20):
    res = texts
    data = tokenizer.texts_to_sequences([texts])[0]
    for i in range(str_len):
        x = to_categorical(data[i: i + inp_words], num_classes=maxWordsCount) 
        inp = x.reshape(1, inp_words, maxWordsCount)
        pred = model.predict(inp)
        indx = pred.argmax(axis=1)[0]
        data.append(indx)
        res += " " + tokenizer.index_word[indx] 
    return res

res = buildPhrase("позитив добавляет годы")
print(res)
```
